{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20c61175",
   "metadata": {},
   "source": [
    "# Meeting Summariser - `gpt-neo-2.7B`\n",
    "\n",
    "This is the notebook for meeting summarizer network built solely with one neural network. It was built on NBX-platform machine with following configuration: `8Core/30GB + T4 GPU`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c2ed40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b655323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int):\n",
    "    import random, numpy as np, torch\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b030ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded ..., moving to GPU\n",
      "CPU times: user 1min 39s, sys: 14.3 s, total: 1min 53s\n",
      "Wall time: 3min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "name = \"EleutherAI/gpt-neo-2.7B\"\n",
    "# get the model and tokenizer, \n",
    "# EleutherAI/gpt-neo-2.7B = 9.9GB compressed\n",
    "# gpt2-xl (1.5Bn Params) = 6.7GB compressed\n",
    "# always cache these models, reduces useless bandwidth.\n",
    "tokenizer = AutoTokenizer.from_pretrained(name, cache_dir = \"../hf-cache/\")\n",
    "model = AutoModelForCausalLM.from_pretrained(name, cache_dir = \"../hf-cache/\")\n",
    "print(\"Model loaded ..., moving to GPU\")\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else \"CPU\"\n",
    "model = model.to(device)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8755dce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caae5cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.4 s, sys: 119 ms, total: 11.5 s\n",
      "Wall time: 14.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# simple forward pass with the model\n",
    "model.eval()\n",
    "\n",
    "prompt = \"\"\"Correct the sentence in each input and return properly formatted sentence\n",
    "###\n",
    "sentence: \"everyone in this part of the world thinks i am a fraud but i know who i am\"\n",
    "correct: \"Everyone in this part of the world thinks I am a fraud, but I know who I am.\"\n",
    "###\n",
    "sentence: \"hey everybody welcome to the all in podcastÂ it was a slow news week so we decided we'd  give you a special episode we're gonna go aroundÂ the horn with our special picks we're each gonna\"\n",
    "correct: \"Hey everybody, welcome to the all in podcast. It was a slow news week so we decided we'd give you a special episode. We're gonna go aroundÂ the horn with our special picks, we're each gonna\"\n",
    "###\n",
    "sentence:\"\"\"\n",
    "\n",
    "query = ''' \"and while india struggles to shake off the virus the developed world is taking off the masks the countries who've been able to vaccinate a sizeable number of people those living in the united states for instance they can ditch the masks at most public places now if they have taken both the shots\"\n",
    "correct:'''\n",
    "\n",
    "input_ids = tokenizer(prompt + query, return_tensors = \"pt\")[\"input_ids\"].to(device)\n",
    "out = model.generate(\n",
    "    input_ids,\n",
    "    max_length = len(input_ids[0]) + 128,\n",
    "    do_sample = True,\n",
    "    early_stopping = True,\n",
    "    temperature = 0.5,\n",
    "    top_p = 0.9,\n",
    "    num_return_sequences=1,\n",
    "    pad_token_id = tokenizer.eos_token_id,\n",
    "\n",
    "    output_scores = False,\n",
    "    output_hidden_states = False,\n",
    "    return_dict_in_generate = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4576ae55",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct the sentence in each input and return properly formatted sentence\n",
      "###\n",
      "sentence: \"everyone in this part of the world thinks i am a fraud but i know who i am\"\n",
      "correct: \"Everyone in this part of the world thinks I am a fraud, but I know who I am.\"\n",
      "###\n",
      "sentence: \"hey everybody welcome to the all in podcastÂ it was a slow news week so we decided we'd  give you a special episode we're gonna go aroundÂ the horn with our special picks we're each gonna\"\n",
      "correct: \"Hey everybody, welcome to the all in podcast. It was a slow news week so we decided we'd give you a special episode. We're gonna go aroundÂ the horn with our special picks, we're each gonna\"\n",
      "###\n",
      "sentence: \"and while india struggles to shake off the virus the developed world is taking off the masks the countries who've been able to vaccinate a sizeable number of people those living in the united states for instance they can ditch the masks at most public places now if they have taken both the shots\"\n",
      "correct: \"and while India struggles to shake off the virus, the developed world is taking off the masks, the countries who've been able to vaccinate a sizeable number of people, those living in the United States for instance, they can ditch the masks at most public places now if they've taken both the shots\"\n",
      "###\n",
      "sentence: \"they've been able to vaccinate a sizeable number of people those living in the united states for instance they can ditch the masks at most public places now if they have taken both the shots\"\n",
      "correct: \"they've been able to vaccinate a sizeable number of people, those living in the\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for x in tokenizer.batch_decode(out.sequences, skip_special_tokens = True):\n",
    "    print(x)\n",
    "    print(\"-\"* 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "d4c5aea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Response():\n",
    "    \"\"\"Class that makes getting generated results chill, simply `print(out)`\"\"\"\n",
    "    def __init__(self, out, t):\n",
    "        self.t = t\n",
    "        self.sequences = out.sequences.cpu().tolist()\n",
    "        self.scores = [x.cpu().numpy() for x in out.scores]  if out.scores != None else None\n",
    "        self.hidden_states = [\n",
    "            [y.cpu().numpy() for y in x]\n",
    "            for x in out.hidden_states\n",
    "        ] if out.hidden_states != None else None\n",
    "        self.attentions = [\n",
    "            [y.cpu().numpy() for y in x]\n",
    "            for x in out.attentions\n",
    "        ] if out.attentions != None else None\n",
    "\n",
    "        self.decoded = self.t.batch_decode(self.sequences, skip_special_tokens = True)\n",
    "\n",
    "    def __repr__(self):\n",
    "        str_ = \"\"\n",
    "        for x in self.decoded:\n",
    "            str_ += x + \"\\n\"\n",
    "            str_ += \"-\"* 70 + \"\\n\"\n",
    "        return str_\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.decoded)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.decoded[i]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for x in self.decoded:\n",
    "            yield x\n",
    "\n",
    "        \n",
    "\n",
    "class GPT():\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.eot_id = tokenizer.eos_token_id\n",
    "\n",
    "    def printout(self, out):\n",
    "        out = out if isinstance(out, torch.Tensor) else out.sequences\n",
    "        for x in self.tokenizer.batch_decode(out, skip_special_tokens = True):\n",
    "            print(x)\n",
    "            print(\"-\"* 70)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        n: int = 16, # number of tokens\n",
    "        r: int = 1,  # number of sequences\n",
    "        do_sample = True,\n",
    "        temp = 0.9,\n",
    "        top_p = 0.9,\n",
    "        top_k = None,\n",
    "        output_scores = None,\n",
    "        output_hidden_states = None,\n",
    "        output_attentions = None,\n",
    "        stop_sequence = None,\n",
    "        return_response = True,\n",
    "        **gen_kwargs\n",
    "    ):\n",
    "        t = self.tokenizer\n",
    "        m = self.model\n",
    "        \n",
    "        # tokenize the input prompt and stop token if provided\n",
    "        input_ids = t(prompt, return_tensors = \"pt\")[\"input_ids\"].to(device)\n",
    "        if stop_sequence is not None:\n",
    "            eos_token_id = t(stop_sequence)[\"input_ids\"][0]\n",
    "        else:\n",
    "            eos_token_id = self.eot_id\n",
    "            \n",
    "        # generate the items\n",
    "        out = m.generate(\n",
    "            input_ids,\n",
    "            max_length = len(input_ids[0]) + n,\n",
    "            temperature = temp,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            num_return_sequences=r,\n",
    "            pad_token_id = self.eot_id,\n",
    "            output_scores = output_scores,\n",
    "            output_hidden_states = output_hidden_states,\n",
    "            output_attentions = output_attentions,\n",
    "            do_sample = do_sample,\n",
    "            early_stopping = True,\n",
    "            return_dict_in_generate = True,\n",
    "            eos_token_id = eos_token_id,\n",
    "            **gen_kwargs\n",
    "        )\n",
    "\n",
    "        # return items or \n",
    "        if return_response:\n",
    "            return Response(out, t)\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "\n",
    "    def classify(\n",
    "        self,\n",
    "        prompt,\n",
    "        labels,\n",
    "        softmax_temp = 0.9,\n",
    "        add_unknown = False,\n",
    "        return_prompt = False,\n",
    "        **gen_kwargs,\n",
    "    ):\n",
    "        # we will use the same format that OpenAI uses for GPT-3\n",
    "        # read: https://beta.openai.com/docs/guides/classifications\n",
    "        # We normalize all labels by `label.strip().lower().capitalize()` at the API\n",
    "        # backend. Thus corresponding output labels are always capitalized.\n",
    "        unq_options = set([x.strip().lower().capitalize() for x in labels])\n",
    "        unq_options = sorted(list(unq_options))\n",
    "\n",
    "        # each label must have a distinct first token, because classification\n",
    "        # works by looking only one step ahead. Also encode the labels with extra\n",
    "        # white space prepended.\n",
    "        label_ids = [tokenizer.encode(\" \" + x)[0] for x in unq_options]\n",
    "        out = self(prompt, **gen_kwargs, n = 1, r = 1, output_scores = True, return_response = False)\n",
    "        logits = out.scores[0][0]\n",
    "        logits = (logits / softmax_temp)[label_ids].softmax(-1).cpu()\n",
    "        logits = logits.numpy()\n",
    "\n",
    "        if add_unknown:\n",
    "            # fill the Probability for the special \"Unknown\" token\n",
    "            scores = {o:i for o,i in zip(unq_options, logits)}\n",
    "            scores[\"Unknown\"] = 1 - sum(scores.values())\n",
    "        else:\n",
    "            scores = {o:i for o,i in zip(unq_options, logits)}\n",
    "        out = {\n",
    "            \"scores\": scores,\n",
    "            \"prompt\": query if return_prompt else None\n",
    "        }\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "f1a25d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = GPT(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72933ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.35 s, sys: 6.22 ms, total: 1.36 s\n",
      "Wall time: 1.38 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2 is a state of the art neural language model, that can predict text from large corpora in a matter of seconds. It has been shown\n",
       "----------------------------------------------------------------------\n",
       "GPT2 is a state of the art neural language model, that can be trained to make good quality predictions for many different tasks including text classification and machine\n",
       "----------------------------------------------------------------------\n",
       "GPT2 is a state of the art neural language model, that can be used in many applications such as speech recognition, translation, and machine translation.\n",
       "----------------------------------------------------------------------"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# simple generation\n",
    "gpt(\"GPT2 is a state of the art neural language model, that can\", n = 16, r = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e90098f6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 241 ms, sys: 23.8 ms, total: 265 ms\n",
      "Wall time: 263 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'scores': {'Negative': 0.7159987, 'Positive': 0.28400132}, 'prompt': None}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# perform classification with a simple built in function\n",
    "gpt.classify(\"\"\"This is a tweet sentiment classifier\n",
    "Tweet: \"I loved the new Batman movie!\"\n",
    "Sentiment: Positive\n",
    "###\n",
    "Tweet: \"I hate it when my phone battery dies ðŸ’¢\"\n",
    "Sentiment: Negative\n",
    "###\n",
    "Tweet: \"My day has been ðŸ‘\"\n",
    "Sentiment: Positive\n",
    "###\n",
    "Tweet: \"This month has been very hard on me\"\n",
    "Sentiment:\"\"\", labels = [\"Positive\", \"Negative\"], add_unknown=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1b6c953d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"rain man david hey everybody welcome to the all in podcast  it was a slow news week so we decided we'd give you a special episode we're gonna go around  the horn with our special picks we're each gonna pick three picks everybody we're gonna pick our  favorite recipe our favorite new hobby and our favorite streaming guilty pleasure because there  was no news uh with us today the dictator chamath palihapatiya rainman david sachs with his new  track from young spielberg just ripping across the charts uh young spielberg added again this time  with a a track focused on the rayman himself and the queen of quinoa who everybody says  we should upgrade to the king of quinoa that's so sexist why is that an upgrade the  queen of quinoa i don't know people just felt i was being i don't know how people could say that  anointing him as queen would be derogatory i think these people are not woke and they need to be  canceled jason here you go again making a lot of assumptions about people's pronouns \""
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summarisation pipeline\n",
    "\n",
    "# - classify the input based on language\n",
    "# - classify the sentences according to quality of english\n",
    "# - if not of good grammar, clean up the sentences\n",
    "# - break into points\n",
    "# - break into keywords\n",
    "\n",
    "from captions import *\n",
    "\n",
    "def get_captions(caption_string):\n",
    "    captions = []\n",
    "    for x in caption_string.split(\"\\n\\n\"):\n",
    "        _id, _time, _content = x.split(\"\\n\")\n",
    "        _time = _time.split(\"-->\")\n",
    "        _from = date_parse(_time[0])\n",
    "        _to = date_parse(_time[1])\n",
    "        # \\xa0 is actually non-breaking space in Latin1 (ISO 8859-1), also chr(160)\n",
    "        _content = _content.replace(u'\\xa0', u' ').strip()\n",
    "        captions.append({\"id\": _id, \"from\": _from, \"to\": _to, \"content\": _content})\n",
    "    return captions\n",
    "\n",
    "\n",
    "with open(\"./sample.srt\") as f:\n",
    "    captions = get_captions(f.read())\n",
    "    \n",
    "capstr = \" \".join([x[\"content\"] for x in captions])\n",
    "capstr[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "77b3705b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hindi', 'Russian', 'English', 'English']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we build flow related functions\n",
    "\n",
    "def check_english_language(sentence):\n",
    "    if isinstance(sentence, list):\n",
    "        return [\n",
    "            check_english_language(s) for s in sentence\n",
    "        ]\n",
    "    prompt = \"\"\"This classifies whether the input sequence into it's language\n",
    "###\n",
    "sentence: \"GPT2 is a state of the art neural language model, that can be trained to make good quality predictions for many different tasks including text classification and machine translation.\"\n",
    "language: English\n",
    "###\n",
    "sentence: \"à¤•à¥‹à¤°à¥‹à¤¨à¤¾ à¤ªà¤° à¤¹à¥‹à¤—à¤¾ à¤¡à¥à¤°à¥‹à¤¨ à¤…à¤Ÿà¥ˆà¤•: ICMR à¤•à¥€ à¤¯à¥‹à¤œà¤¨à¤¾- à¤¦à¥à¤°à¥à¤—à¤® à¤‡à¤²à¤¾à¤•à¥‹à¤‚ à¤®à¥‡à¤‚ à¤¡à¥à¤°à¥‹à¤¨ à¤¸à¥‡ à¤¹à¥‹à¤—à¥€ à¤µà¥ˆà¤•à¥à¤¸à¥€à¤¨ à¤•à¥€ à¤¡à¤¿à¤²à¥€à¤µà¤°à¥€, à¤¤à¥‡à¤²à¤‚à¤—à¤¾à¤¨à¤¾ à¤¸à¤°à¤•à¤¾à¤° à¤¨à¥‡ à¤à¤¸à¤¾ à¤ªà¥à¤°à¥‹à¤œà¥‡à¤•à¥à¤Ÿ à¤²à¥‰à¤¨à¥à¤š à¤•à¤¿à¤¯à¤¾\"\n",
    "language: Hindi\n",
    "###\n",
    "sentence: \"Ñ„Ð°Ð½Ñ‚Ð°ÑÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ñ€Ð¾Ð¼Ð°Ð½ ÐÐ»ÐµÐºÑÐµÑ ÐÐ¸ÐºÐ¾Ð»Ð°ÐµÐ²Ð¸Ñ‡Ð° Ð¢Ð¾Ð»ÑÑ‚Ð¾Ð³Ð¾ Ð¾ Ð¿ÑƒÑ‚ÐµÑˆÐµÑÑ‚Ð²Ð¸Ð¸ Ð·ÐµÐ¼Ð»ÑÐ½ Ð½Ð° ÐœÐ°Ñ€Ñ. Ð¢ÐµÐºÑÑ‚ Ð½Ð°Ð¿Ð¸ÑÐ°Ð½ Ð² Ð¾ÑÐ½Ð¾Ð²Ð½Ð¾Ð¼ Ð² ÑÐ¼Ð¸Ð³Ñ€Ð°Ñ†Ð¸Ð¸, Ð¿ÐµÑ€Ð²Ð¾Ðµ Ð¸Ð·Ð´Ð°Ð½Ð¸Ðµ Ð²Ñ‹ÑˆÐ»Ð¾ Ð² ÐŸÐµÑ‚Ñ€Ð¾Ð³Ñ€Ð°Ð´Ðµ Ð² 1923 Ð³Ð¾Ð´Ñƒ Ð¸ Ð½ÐµÐ¾Ð´Ð½Ð¾ÐºÑ€Ð°Ñ‚Ð½Ð¾ Ð¿ÐµÑ€ÐµÐ¿ÐµÑ‡Ð°Ñ‚Ñ‹Ð²Ð°Ð»Ð¾ÑÑŒ.\"\n",
    "language: Russian\n",
    "###\n",
    "sentence: {sentence}\n",
    "language:\"\"\"\n",
    "    p = prompt.format(sentence = sentence)\n",
    "    out = gpt(p, n = 4, r = 1, stop_sequence=\"###\", temp=0.7, do_sample = False)[0]\n",
    "    out = out[len(p):].strip().split(\"\\n\")[0]\n",
    "    return out\n",
    "\n",
    "# checks\n",
    "c1 = check_english_language([\n",
    "    'à¤¬à¤¦à¤²à¥‡à¤—à¥€ à¤¬à¤¿à¤¹à¤¾à¤° à¤•à¥€ à¤¸à¤¿à¤¯à¤¾à¤¸à¤¤: à¤ªà¤¶à¥à¤ªà¤¤à¤¿ à¤ªà¤¾à¤°à¤¸ à¤•à¥‡à¤¨à¥à¤¦à¥à¤° à¤®à¥‡à¤‚ à¤®à¤‚à¤¤à¥à¤°à¥€ à¤¬à¤¨ à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚; à¤–à¥à¤¦ à¤•à¥‹ BJP à¤•à¤¾ à¤¹à¤¨à¥à¤®à¤¾à¤¨ à¤•à¤¹à¤¨à¥‡ à¤µà¤¾à¤²à¥‡ à¤šà¤¿à¤°à¤¾à¤— à¤•à¥‹ RJD à¤”à¤° à¤•à¤¾à¤‚à¤—à¥à¤°à¥‡à¤¸ à¤¨à¥‡ à¤¦à¤¿à¤¯à¤¾ à¤‘à¤«à¤°',\n",
    "    'Ð¢Ð¾Ð»ÑÑ‚Ð¾Ð³Ð¾, Ð²Ð¾Ð¿Ð»Ð¾Ñ‰Ð°Ð²ÑˆÐ°Ñ ÐµÐ³Ð¾ Ñ…Ð°Ñ€Ð°ÐºÑ‚ÐµÑ€Ð½Ñ‹Ðµ ÑÑ‚Ð¸Ð»Ð¸ÑÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð¸ Ð¸Ð´ÐµÐ¾Ð»Ð¾Ð³Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð¼Ð¾Ñ‚Ð¸Ð²Ñ‹, Ð²Ñ‹Ñ€Ð°Ð¶Ð°Ð²ÑˆÐ°Ñ Ñ‚Ð²Ð¾Ñ€Ñ‡ÐµÑÐºÑƒÑŽ ÑÐ²Ð¾Ð±Ð¾Ð´Ñƒ Ð¸ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ÑÑ‚ÑŒ Ð¶Ð°Ð½Ñ€Ð¾Ð²Ð¾-ÑÑ‚Ð¸Ð»Ð¸ÑÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… ÑÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ð¾Ð². ÐŸÐ¾ÑÐ»Ðµ 1939 Ð³Ð¾Ð´Ð° Ð¿Ð¾Ð²ÐµÑÑ‚ÑŒ Ð¿Ð¾ÑÑ‚Ð¾ÑÐ½Ð½Ð¾ Ð¿ÐµÑ€ÐµÐ¸Ð·Ð´Ð°Ñ‘Ñ‚ÑÑ Ð¸ Ð¿Ñ€ÐµÐ²Ñ€Ð°Ñ‚Ð¸Ð»Ð°ÑÑŒ Ð² ÐºÐ»Ð°ÑÑÐ¸ÐºÑƒ Ð´ÐµÑ‚ÑÐºÐ¾-ÑŽÐ½Ð¾ÑˆÐµÑÐºÐ¾Ð¹ Ð»Ð¸Ñ‚ÐµÑ€Ð°Ñ‚ÑƒÑ€Ñ‹.',\n",
    "    'è¯·æ‚¨è¯´å¾—æ…¢äº›å¥½å—',\n",
    "    \" \".join(capstr.split()[:54]) #### --> this should be English\n",
    "])\n",
    "c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7a351a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scores': {'Correct': 0.8130481, 'Wrong': 0.18695185}, 'prompt': None}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_good_english(sentence):\n",
    "    good_english_prompt = '''This corrects the input sentence\n",
    "###\n",
    "sentence: \"everyone in this part of the world thinks i am a fraud but i know who i am\"\n",
    "grammar: Wron\n",
    "###\n",
    "sentence: \"I loved the new Batman movie! It was really really good\"\n",
    "grammar: Correct\n",
    "###\n",
    "sentence: \"HI, I AM HERE. SITTING IN THE RAIN, WHILE THE WORLD SLEEPS\"\n",
    "grammar: Wrong\n",
    "###\n",
    "sentence: \"{sentence}\"\n",
    "grammar:'''\n",
    "    p = good_english_prompt.format(sentence = sentence)\n",
    "    out = gpt.classify(\n",
    "        p,\n",
    "        labels = [\"Correct\", \"Wrong\"],\n",
    "        add_unknown=False,\n",
    "        temp = 0.7,\n",
    "        do_sample = True\n",
    "    )\n",
    "    return out\n",
    "    \n",
    "sentence = \"and while india struggles to shake off the virus the developed world is taking off the masks the countries who've been able to vaccinate a sizeable number of people those living in the united states for instance they can ditch the masks at most public places now if they have taken both the shots\"\n",
    "is_good_english(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b43a0add",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "def format_sentence(sentence):\n",
    "    if isinstance(sentence, list):\n",
    "        out = []\n",
    "        for s in sentence:\n",
    "            out.append(format_sentence(s))\n",
    "            sleep(0.5)\n",
    "        return out\n",
    "\n",
    "    # try keeping len(sentence.split()) ~ 54\n",
    "    set_seed(90)\n",
    "    prompt = \"\"\"Correct the sentence in each input and return properly formatted sentence\n",
    "###\n",
    "sentence: \"everyone in this part of the world thinks i am a fraud but i know who i am.\"\n",
    "correct: \"Everyone in this part of the world thinks I am a fraud, but I know who I am.\"\n",
    "###\n",
    "sentence: \"hey everybody welcome to the all in podcastÂ it was a slow news week so we decided we'd  give you a special episode we're gonna go aroundÂ the horn with our special picks we're each gonna.\"\n",
    "correct: \"Hey everybody, welcome to the all in podcast. It was a slow news week so we decided we'd give you a special episode. We're gonna go aroundÂ the horn with our special picks, we're each gonna\"\n",
    "###\n",
    "sentence:\"{sentence}\"\n",
    "correct:\"\"\"\n",
    "\n",
    "    p = prompt.format(sentence = sentence)\n",
    "    n = len(gpt.tokenizer.tokenize(sentence)) + 10 # margin of error\n",
    "    g = gpt(\n",
    "        p,\n",
    "        n,\n",
    "        r=1,\n",
    "        stop_sequence = \"\\n\",\n",
    "        temp = 1.0,\n",
    "        top_p = 1.0\n",
    "    )[0]\n",
    "    \n",
    "    # clean up the response\n",
    "    s = g.split(\"###\")[-2 if g.endswith('###') else -1]\n",
    "    res = s.split(\"\\n\")[2][8:].replace('\"', '').strip()\n",
    "    return res[:-1]\n",
    "  \n",
    "# corr = 0\n",
    "# for _ in range(10):\n",
    "#     r = format_sentence([\n",
    "#         \"and while india struggles to shake off the virus the developed world is taking off the masks the countries who've been able to vaccinate a sizeable number of people those living in the united states for instance they can ditch the masks at most public places now if they have taken both the shots.\",\n",
    "#         \" \".join(capstr.split()[:54]) + \".\",\n",
    "#         \" \".join(capstr.split()[150:250]) + \".\"\n",
    "#     ])[-1]\n",
    "#     print(\"-\" * 70)\n",
    "#     print(r)\n",
    "#     x = int(r == \"As queen would be derogatory I think these people are not woke and they need to be canceled, Jason here you go again making a lot of assumptions about people's pronouns, Yeah they they queen of quinoa they I take no offense I take no offense your insults to me and uh today I'm having the emotion of excitement and I am ready for the conversation, good we got the firmware upgraded all right so I think we might as well start with I don't know if you guys caught this but there's a red subreddit called\")\n",
    "#     print(x)\n",
    "#     corr += x\n",
    "# corr / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "a179866c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Wall Street bets and what they do on wall street bets is they find angles and an thesis and then they bet on a stock the stock they picked for the past couple of months has been gamestop and boy jason hold on a second um that's that's that's not true so um do you uh i had actually a guy on my team put together two important documents and i'm just going to read them because it's full of so much interesting and then we can talk about where are these from did you o\""
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = format_sentence(\" \".join(capstr.split()[250:350]) + \".\")\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "0648a97f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stock options',\n",
       " 'stock prices',\n",
       " 'hold',\n",
       " 'stock',\n",
       " 'stock trading',\n",
       " 'wall street',\n",
       " 'thesis',\n",
       " 'stock news',\n",
       " 'stock market',\n",
       " 'bet',\n",
       " 'stock quotes',\n",
       " 'stock broker',\n",
       " 'stock market crash',\n",
       " 'document',\n",
       " 'stock trading tips',\n",
       " 'gambling']"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_keywords(text, r = 4):\n",
    "    prompt = \"\"\"Get keywords from each text\n",
    "\n",
    "###\n",
    "\n",
    "Text: \"Black-on-black ware is a 20th- and 21st-century pottery tradition developed by the Puebloan Native American ceramic artists in Northern New Mexico. Traditional reduction-fired blackware has been made for centuries by pueblo artists. Black-on-black ware of the past century is produced with a smooth surface, with the designs applied through selective burnishing or the application of refractory slip. Another style involves carving or incising designs and selectively polishing the raised areas. For generations several families from Kha'po Owingeh and P'ohwhÃ³ge Owingeh pueblos have been making black-on-black ware with the techniques passed down from matriarch potters. Artists from other pueblos have also produced black-on-black ware. Several contemporary artists have created works honoring the pottery of their ancestors.\"\n",
    "\n",
    "Keywords: Pueblo, art, pottery, black, black ware\n",
    "\n",
    "###\n",
    "\n",
    "Text: \"{text}\"\n",
    "\n",
    "keywords:\"\"\"\n",
    "\n",
    "    p = prompt.format(text = text)\n",
    "    words = set()\n",
    "    \n",
    "    # prevents GPU OOM by batching instead of parallel requests\n",
    "    for _ in range(0, r+1, 2):\n",
    "        out = gpt(\n",
    "            p,\n",
    "            r = 2,\n",
    "            n = 20,\n",
    "            stop_sequence=\"\\n\",\n",
    "            temp = 0.9,\n",
    "            repetitive_penalty = 0.9 # don't repeat the same thing over and over again\n",
    "        )\n",
    "\n",
    "        for s in out:\n",
    "            ws = s.split(\"###\")[2].split(\"\\nkeywords:\")[-1].split(\",\")\n",
    "            for w in ws:\n",
    "                w = w.strip().lower()\n",
    "                # print(\"--->\", w, w in [\"pueblo\", \"art\", \"pottery\", \"black\", \"black ware\"])\n",
    "                if w and len(w.split()) < 4 and w not in [\"pueblo\", \"art\", \"pottery\", \"black\", \"black ware\"]:\n",
    "                    words.add(w)\n",
    "    return list(words)\n",
    "        \n",
    "get_keywords(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "a473370a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Got sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [01:02<00:00, 12.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 51s, sys: 4.81 s, total: 1min 56s\n",
      "Wall time: 1min 59s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cap_str = [\n",
    "    (\" \".join(capstr.split()[i:i+100]) + \".\")\n",
    "    for i in range(550, 1050, 100)\n",
    "]\n",
    "outs = format_sentence(cap_str)\n",
    "print(\"---> Got sentences.\")\n",
    "words = []\n",
    "for _, o in zip(trange(len(outs)), outs):\n",
    "    words.append(get_keywords(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "0f880cb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['reddity',\n",
       "  'redditt',\n",
       "  'a red list',\n",
       "  'the',\n",
       "  'redditor',\n",
       "  'culture',\n",
       "  'community',\n",
       "  'redditor list',\n",
       "  'a redditor',\n",
       "  'reddiquette',\n",
       "  'redditr',\n",
       "  'queer',\n",
       "  'r/quinoa',\n",
       "  'quinoa',\n",
       "  'reddit',\n",
       "  'redditer',\n",
       "  'reddits'],\n",
       " ['street',\n",
       "  'wall',\n",
       "  'stock',\n",
       "  'wall street',\n",
       "  'hedge fund',\n",
       "  'stock pick',\n",
       "  'thesis',\n",
       "  'stock analyst',\n",
       "  'bet',\n",
       "  'obey',\n",
       "  'angle',\n",
       "  'stock tip',\n",
       "  'quote',\n",
       "  'new york',\n",
       "  'gamestop'],\n",
       " ['dictatorship',\n",
       "  'corporatist',\n",
       "  'financial',\n",
       "  'dictator',\n",
       "  'black-on-black ware',\n",
       "  'scumb',\n",
       "  'wall street',\n",
       "  'corporatism',\n",
       "  'goldman sachs',\n",
       "  'wall street crash',\n",
       "  'robin hood',\n",
       "  \"ceo's compensation\",\n",
       "  'bats',\n",
       "  'ceo',\n",
       "  'black-on-',\n",
       "  \"ceo's\"],\n",
       " ['podcasts',\n",
       "  'podcast',\n",
       "  'episode',\n",
       "  'all in',\n",
       "  'all i',\n",
       "  'special podcast',\n",
       "  'all in podcast',\n",
       "  'all ian',\n",
       "  'in',\n",
       "  '\"all i\"']]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "72e1eab4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-->> stock options, share, stocks, ban, stock, wall street, block, securities, wall stree, stock market, markets, 2019, many, market, trading, june\n",
      "['', 'stock options', 'share', 'stocks', 'wall', 'stock', 'ban', 'wall street', 'block', 'securities', 'wall stree', 'stock market', 'markets', '2019', 'many', 'market', 'trading', 'june']\n",
      "-->> stocks, stock, stock trading, money, texas, forum, stock market, bet, market, investing, trading\n",
      "['texas', 'forum', 'stock market', 'bet', 'market', 'investing', 'trading']\n",
      "-->> video, deep value, fund, momentum, screenshot, hedge fund, capital, investment, dislocations, strategy, market, hedge fund capital, hedge funds\n",
      "['fund', 'momentum', 'hedge fund', 'screenshot', 'capital', 'investment', 'dislocations', 'market', 'hedge fund capital', 'hedge funds']\n",
      "-->> position, gme yolo, gme yolo update, michael, august, free cash flow, this guy, company, update, bury, value oriented thesis, big short, yolo, michael bury, is, gme, alex michael bury, 2019, buyback, value, who, fundamental thesis, 22nd\n",
      "['value oriented thesis', 'position', 'gme yolo', 'gme yolo update', 'big short', 'yolo', 'michael', 'who', 'fundamental thesis', 'this guy', 'alex michael bury']\n",
      "-->> video, retail, stock, retail store, stock price, store, stock market, mall, console, gamestop corporation, retailing, video games, gamestop\n",
      "['video', 'retail', 'stock', 'retail store', 'stock price', 'store', 'mall', 'console', 'retailing', 'gamestop corporation', 'video games', 'gamestop']\n"
     ]
    }
   ],
   "source": [
    "def clean_keywords(keywords):\n",
    "    \n",
    "    if isinstance(keywords[0], list):\n",
    "        return [clean_keywords[x] for x in keywords]\n",
    "    \n",
    "    \n",
    "    # make the keywords a sentence for prompt\n",
    "    k = \", \".join(keywords)\n",
    "    k = re.sub(r\"[^\\w\\'\\s,]\", \"\", k)\n",
    "        \n",
    "    prompt = '''This app removed duplicate words from a list of words\n",
    "\n",
    "###\n",
    "\n",
    "Sentence: \"reddity, redditt, a red list, the, redditor, culture, community, redditor list, a redditor, reddiquette, redditr, queer, r/quinoa, quinoa, reddit, redditer, reddits\"\n",
    "\n",
    "Important words: reddit, culture, community, quinoa, r/quinoa\n",
    "\n",
    "###\n",
    "\n",
    "Setence: \"podcasts, podcast, episode, all in, all i, special podcast, all in podcast, all ian, in, all i\"\n",
    "\n",
    "Important words: all in podcast\n",
    "\n",
    "###\n",
    "\n",
    "Sentence: \"street, wall, stock, wall street, hedge fund, stock pick, thesis, stock analyst, bet, obey, angle, stock tip, quote, new york, gamestop\"\n",
    "\n",
    "Important words: wall street, hedge fund, new york, gamestop\n",
    "\n",
    "###\n",
    "\n",
    "Sentence: {sentence}\n",
    "\n",
    "Important words:'''\n",
    "    \n",
    "    print(\"-->>\", k)\n",
    "    \n",
    "    p = prompt.format(sentence = k)\n",
    "\n",
    "    out = gpt(\n",
    "        p,\n",
    "        n = 32,\n",
    "        r = 5,\n",
    "        temp = 1.0,\n",
    "        top_p = 0.9,\n",
    "        stop_sequence=\"\\n\",\n",
    "    )\n",
    "    \n",
    "    words = set()\n",
    "    for x in out:\n",
    "        ws = x.split(\"###\")[-1].split(\"Important words:\")[-1].strip().split(\",\")\n",
    "        for w in ws:\n",
    "            words.add(w.strip())\n",
    "    return list(words)\n",
    "    \n",
    "    \n",
    "w2 = []\n",
    "for w in words:\n",
    "    out = clean_keywords(w)\n",
    "    print(out)\n",
    "    w2.append(out)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d329f4eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML and DL with CUDA 10.2",
   "language": "python",
   "name": "cuda101"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
