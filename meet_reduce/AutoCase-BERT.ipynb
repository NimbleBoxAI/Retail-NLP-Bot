{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a32d7623",
   "metadata": {},
   "source": [
    "# AutoCasing - BERT\n",
    "\n",
    "This notebook has one of the approaches used for training a NN for performing automatic casing for all lower case text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fb71dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.31 s, sys: 763 ms, total: 2.07 s\n",
      "Wall time: 2.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModel, BertTokenizerFast\n",
    "\n",
    "from tqdm import trange\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "from daily import *\n",
    "from collate_methods import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d24cd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset reddit (../reddit/default/1.0.0/98ba5abea674d3178f7588aa6518a5510dc0c6fa8176d9653a3546d5afcb3969)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 204 ms, sys: 22.6 ms, total: 227 ms\n",
      "Wall time: 5.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataset = load_dataset(\"reddit\", cache_dir=\"../\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\", cache_dir = \"../hf-cache/\")\n",
    "# device = torch.device(\"cuda:0\") if torch.cuda.is_available() else \"cpu\"\n",
    "# model = AutoModel.from_pretrained(\"bert-base-cased\", cache_dir = \"../hf-cache/\", ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3b4017c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['author', 'body', 'normalizedBody', 'subreddit', 'subreddit_id', 'id', 'content', 'summary'],\n",
       "        num_rows: 3848330\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcfaa5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at ../reddit/default/1.0.0/98ba5abea674d3178f7588aa6518a5510dc0c6fa8176d9653a3546d5afcb3969/cache-e5dc09252e8e1642.arrow and ../reddit/default/1.0.0/98ba5abea674d3178f7588aa6518a5510dc0c6fa8176d9653a3546d5afcb3969/cache-015e7bd21d0fb866.arrow\n"
     ]
    }
   ],
   "source": [
    "train_split_ratio = 0.99\n",
    "seed = 4\n",
    "dd = dataset[\"train\"].train_test_split(\n",
    "  test_size = 1 - train_split_ratio,\n",
    "  train_size = train_split_ratio,\n",
    "  seed = seed\n",
    ")\n",
    "dstrain = dd[\"train\"]\n",
    "dstest = dd[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2daa889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['author', 'body', 'normalizedBody', 'subreddit', 'subreddit_id', 'id', 'content', 'summary'],\n",
       "     num_rows: 3809846\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['author', 'body', 'normalizedBody', 'subreddit', 'subreddit_id', 'id', 'content', 'summary'],\n",
       "     num_rows: 38484\n",
       " }))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dstrain, dstest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34e69289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'Hello', 'World', '[SEP]']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [tokenizer.decode(x) for x in tokenizer(\"Hello World!\")[\"input_ids\"]]\n",
    "tokenizer.tokenize(\"Hello World\", add_special_tokens = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a22c1224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 389 ms, sys: 149 ms, total: 537 ms\n",
      "Wall time: 536 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x = tokenizer(\n",
    "        \"Luca Brasi held a gun to his head, and my father assured\"\n",
    "        \" him that either his brains or his signature would be on\"\n",
    "        \" the contract. Thatâ€™s a true story. - Michael Corleone, Godfather\",\n",
    "        return_tensors = \"pt\"\n",
    "    )\n",
    "    out = model(**{k:v.to(device) for k,v in x.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7434bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 46, 768])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86f58f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"bert-base-cased\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.5.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 28996\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1781858c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loader init ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (600 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (615 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1465 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (766 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (851 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1163 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0949)\n",
      "tensor(0.1015)\n",
      "tensor(0.0876)\n",
      "tensor(0.0824)\n",
      "tensor(0.0921)\n",
      "tensor(0.0957)\n",
      "tensor(0.0906)\n",
      "tensor(0.0941)\n",
      "tensor(0.0898)\n",
      "tensor(0.0900)\n",
      "tensor(0.1102)\n",
      "tensor(0.1002)\n",
      "tensor(0.1070)\n",
      "tensor(0.0961)\n",
      "tensor(0.0874)\n",
      "tensor(0.0943)\n",
      "tensor(0.0843)\n",
      "tensor(0.0965)\n",
      "tensor(0.0954)\n",
      "tensor(0.1089)\n",
      "tensor(0.0915)\n",
      "tensor(0.1004)\n",
      "tensor(0.1052)\n",
      "tensor(0.0946)\n",
      "tensor(0.0808)\n",
      "tensor(0.0995)\n",
      "tensor(0.1000)\n",
      "tensor(0.1029)\n",
      "tensor(0.0906)\n",
      "tensor(0.0981)\n",
      "tensor(0.0959)\n",
      "tensor(0.0969)\n",
      "tensor(0.1043)\n",
      "tensor(0.0914)\n",
      "tensor(0.1023)\n",
      "tensor(0.1000)\n",
      "tensor(0.0952)\n",
      "tensor(0.1028)\n",
      "tensor(0.0799)\n",
      "tensor(0.1018)\n",
      "tensor(0.1104)\n",
      "tensor(0.0948)\n",
      "tensor(0.1025)\n",
      "tensor(0.0957)\n",
      "tensor(0.0916)\n",
      "tensor(0.1083)\n",
      "tensor(0.0892)\n",
      "tensor(0.0974)\n",
      "tensor(0.0855)\n",
      "tensor(0.1040)\n",
      "tensor(0.1026)\n",
      "tensor(0.0926)\n",
      "tensor(0.0944)\n",
      "tensor(0.1074)\n",
      "tensor(0.0811)\n",
      "tensor(0.0905)\n",
      "tensor(0.0843)\n",
      "tensor(0.0766)\n",
      "tensor(0.1031)\n",
      "tensor(0.1125)\n",
      "tensor(0.1021)\n",
      "tensor(0.0960)\n",
      "tensor(0.0991)\n",
      "tensor(0.1013)\n",
      "tensor(0.0863)\n",
      "tensor(0.0877)\n",
      "tensor(0.0947)\n",
      "tensor(0.0971)\n",
      "tensor(0.0925)\n",
      "tensor(0.1049)\n",
      "tensor(0.0845)\n",
      "tensor(0.0883)\n",
      "tensor(0.0959)\n",
      "tensor(0.0987)\n",
      "tensor(0.1027)\n",
      "tensor(0.1034)\n",
      "tensor(0.0939)\n",
      "tensor(0.1041)\n",
      "tensor(0.0958)\n",
      "tensor(0.1022)\n",
      "tensor(0.0947)\n",
      "tensor(0.1096)\n",
      "tensor(0.0963)\n",
      "tensor(0.1013)\n",
      "tensor(0.1012)\n",
      "tensor(0.0953)\n",
      "tensor(0.1041)\n",
      "tensor(0.1109)\n",
      "tensor(0.0916)\n",
      "tensor(0.1063)\n",
      "tensor(0.1005)\n",
      "tensor(0.0846)\n",
      "tensor(0.0992)\n",
      "tensor(0.0936)\n",
      "tensor(0.0930)\n",
      "tensor(0.0922)\n",
      "tensor(0.1131)\n",
      "tensor(0.1116)\n",
      "tensor(0.0870)\n",
      "tensor(0.1101)\n",
      "tensor(0.0818)\n",
      "CPU times: user 1.35 s, sys: 357 ms, total: 1.71 s\n",
      "Wall time: 13.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from autocase.collate_methods import *    \n",
    "\n",
    "# NOTE: since these collator functions sit outside the dataset it is difficult\n",
    "# to know what exactly will be the final batch size generally 25% smaller than\n",
    "# the batch_size given as an input to the model.\n",
    "\n",
    "COLLATE_METHODS = get_collate_fns(tokenizer, 512)\n",
    "collate_fn = COLLATE_METHODS[\"fast_binary_flag\"]\n",
    "\n",
    "# collate_fn = FastBinaryCollater(tokenizer, 512)\n",
    "    \n",
    "loader = DataLoader(\n",
    "    dstrain,\n",
    "    batch_size=64,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory = True if torch.cuda.is_available() else False,\n",
    "    shuffle = False,\n",
    "    num_workers = 6\n",
    ")\n",
    "print(\"Loader init ...\")\n",
    "\n",
    "for i, x in enumerate(loader):\n",
    "    print(torch.sum(x[\"labels\"]) / torch.numel(x[\"labels\"]))\n",
    "    if i == 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "058da5a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As The title says i haven\\'t had pretty much any sexual contact except for maybe a kiss since i was a college freshman back when i was a naive 18 year old after a few setbacks and a long battle with depression i finally decided to get back into the game maybe i\\'ve lost my touch or i\\'m just too busy with work but i don\\'t have time for a relationship and just want a pal who can make my toes curl do guys do that too ? i\\'m just you\\'re average 21 year old i won\\'t say i\\'m something special or a model i have an average to slim build kind of have resting bitch face i\\'ve tried fixing it sorry and as for my \" measurements \" i would say pretty average as well but doesn\\'t every guy think that ? i don\\'t really have an set conditions except be clean please sane please don\\'t stalk me and hopefully be relatively close looking to host or uber and not looking to make this experience expensive for either of us as for age and appearance i have no preference but if you\\'re older i might like you more since i have a fantasy to have an older woman seduce me heard good things about this sub so hope to hear from y\\'all tldr average 21 year old hasn\\'t had head in 3 years and is hoping no one crazy bites my dick off edit searching for as long as the post is up'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_cased = []\n",
    "for s,d in zip(x[\"input_ids\"], x[\"labels\"]):\n",
    "    _s = []\n",
    "    s = tokenizer.tokenize(\n",
    "        tokenizer.decode(s, skip_special_tokens = True),\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "    for i,t in enumerate(s):\n",
    "        if i in d:\n",
    "            _s.append(t.capitalize())\n",
    "        else:\n",
    "            _s.append(t)\n",
    "    auto_cased.append(_s)\n",
    "    \n",
    "fseq = \"\"\n",
    "for t in _s:\n",
    "    fseq += t if t[0] == \"#\" else \" \" + t\n",
    "fseq = re.sub(r\"#\", \"\", fseq.strip())\n",
    "fseq = re.sub(r\"\\s'\\s\", \"'\", fseq)\n",
    "fseq = re.sub(r\"\\[\\s\", \"[\", fseq)\n",
    "fseq = re.sub(r\"\\s\\]\", \"]\", fseq)\n",
    "fseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c32c807f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As the title says I haven\\'t had pretty much any sexual contact except for maybe a kiss since I was a college freshman back when I was a naive 18 year old After a few setbacks and a long battle with depression I finally decided to get back into the game Maybe I\\'ve lost my touch or I\\'m just too busy with work but I don\\'t have time for a relationship and just want a pal who can make my toes curl do guys do that too ? I\\'m just you\\'re average 21 year old I won\\'t say I\\'m something special or a model I have an average to slim build Kind of have resting bitch face I\\'ve tried fixing it sorry and as for my \" measurements \" I would say pretty average as well but doesn\\'t every guy think that ? I don\\'t really have an set conditions except be clean please Sane please don\\'t stalk me And hopefully be relatively close Looking to host or Uber and not looking to make this experience expensive for either of us As for age and appearance I have no preference but if you\\'re older I might like you more since I have a fantasy to have an older woman seduce me Heard good things about this sub so hope to hear from y\\'all TLDR average 21 year old hasn\\'t had head in 3 years and is hoping no one crazy bites my dick off Edit searching for as long as the post is up'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tseq = \"\"\n",
    "for t in x[\"target_str\"][-1]:\n",
    "    tseq += t if t[0] == \"#\" else \" \" + t\n",
    "tseq = re.sub(r\"#\", \"\", tseq.strip())\n",
    "tseq = re.sub(r\"\\s'\\s\", \"'\", tseq)\n",
    "tseq = re.sub(r\"\\[\\s\", \"[\", tseq)\n",
    "tseq = re.sub(r\"\\s\\]\", \"]\", tseq)\n",
    "tseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e498c21e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['labels', 'target_str', 'input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2f659bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is not perfect but still okay for this demo case also we'll add a whole bunch of regex's\n",
    "# and get all this sorted out. So now we make the model for this case\n",
    "\n",
    "class AutoCaseModel(nn.Module):\n",
    "    def __init__(self, hf_backbone = \"bert-base-cased\", cache_dir = \"../hf-cache/\"):\n",
    "        super().__init__()\n",
    "        assert \"bert\" in hf_backbone.lower(), \"Supports only BERT Models\"\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(hf_backbone, cache_dir = cache_dir)\n",
    "        self.device = torch.device(\"cuda:0\") if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = AutoModel.from_pretrained(hf_backbone, cache_dir = cache_dir).to(self.device)\n",
    "        self.model.eval()\n",
    "        self.model_config = self.model.config\n",
    "        \n",
    "        self.head = nn.Linear(self.model.config.hidden_size, 1).to(self.device)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels = None, *args, **kwargs):\n",
    "        with torch.no_grad():\n",
    "            attention_mask = attention_mask.to(self.device)\n",
    "            out = self.model(**{\n",
    "                \"input_ids\": input_ids.to(self.device),\n",
    "                \"attention_mask\": attention_mask\n",
    "            }).last_hidden_state\n",
    "        logits_2d = torch.sigmoid(self.head(out))\n",
    "        \n",
    "        if labels is not None:\n",
    "            # load labels on the model device\n",
    "            labels = labels.to(self.device)\n",
    "            \n",
    "            # we need to remove the tokens where attention_mask != 1\n",
    "            attn_flat = attention_mask.contiguous().view(-1)\n",
    "            attn_mask = attn_flat > 0\n",
    "            logits = logits_2d.contiguous().view(-1)[attn_mask]\n",
    "            labels = labels.contiguous().view(-1)[attn_mask].float()\n",
    "            \n",
    "            # create a weight matrix for the loss\n",
    "            weight_mat = torch.zeros_like(logits).float()\n",
    "            weight_mat[labels == 1] = 4\n",
    "            weight_mat += 1\n",
    "            loss = F.binary_cross_entropy(\n",
    "                logits,\n",
    "                labels,\n",
    "                weight_mat\n",
    "            )\n",
    "            \n",
    "            # calculate accuracy right in here\n",
    "            # if x > 0.5 -> 1\n",
    "            logits_act = torch.zeros_like(logits)\n",
    "            logits_act[logits > 0.5] = 1\n",
    "            corr = logits_act == labels\n",
    "            corr = torch.sum(corr) / torch.numel(corr)\n",
    "\n",
    "            f1 = f1_score(labels.cpu(), logits_act.cpu())\n",
    "            cm = confusion_matrix(labels.cpu(), logits_act.cpu())\n",
    "            \n",
    "            return (logits_2d, loss, corr.item(), f1, cm)\n",
    "        return logits_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fe66261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.2 s, sys: 1.41 s, total: 23.6 s\n",
      "Wall time: 28.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = AutoCaseModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "943fe47a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/envs/cuda111/lib/python3.8/site-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 512, 1]),\n",
       " tensor(1.2001, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>),\n",
       " 0.7595351934432983,\n",
       " 0.12797502926258292,\n",
       " array([[13791,  1688],\n",
       "        [ 2782,   328]]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits, loss, acc, f1, cm = model(**x)\n",
    "logits.shape, loss, acc, f1, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "29d8bfe0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loader init ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2378 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (517 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (719 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (544 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (826 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (626 > 512). Running this sequence through the model will result in indexing errors\n",
      "Tr Ac: 0.795 | Tr Lo: 0.735 | Tr F1: 0.579 | Te Ac: nan | Te Lo: nan | Te F1: nan | Tr TN: 0.795 | Te TN: 0.000:   0%|          | 362/100000 [28:37<131:18:01,  4.74s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-233d7a36df9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;31m# perform training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/envs/cuda111/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-a067ba7516a2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, labels, *args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;31m# load labels on the model device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;31m# we need to remove the tokens where attention_mask != 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# fuck the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# create optimizer\n",
    "optim = torch.optim.Adam(model.head.parameters())\n",
    "\n",
    "# create train and test data loader\n",
    "COLLATE_METHODS = get_collate_fns(model.tokenizer, 512)\n",
    "collate_fn = COLLATE_METHODS[\"fast_binary_flag\"]\n",
    "train_dl = DataLoader(\n",
    "    dstrain,\n",
    "    batch_size=128,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory = True if torch.cuda.is_available() else False,\n",
    "    shuffle = True,\n",
    "    num_workers = 6\n",
    ")\n",
    "dl = iter(train_dl) # and create multiple iterators\n",
    "test_dl = DataLoader(\n",
    "    dstest,\n",
    "    batch_size = 256,\n",
    "    collate_fn = collate_fn,\n",
    "    pin_memory = True if torch.cuda.is_available() else False,\n",
    "    num_workers = 6\n",
    ")\n",
    "print(\"Loader init ...\")\n",
    "\n",
    "# create lists\n",
    "train_acc = []\n",
    "train_loss = []\n",
    "train_cm = []\n",
    "train_f1 = []\n",
    "\n",
    "test_acc = []\n",
    "test_loss = []\n",
    "test_cm = []\n",
    "test_f1 = []\n",
    "\n",
    "# create controls\n",
    "n_steps = 100000\n",
    "n_steps_test = 1000\n",
    "\n",
    "pbar = trange(n_steps)\n",
    "for gs in pbar:\n",
    "    # train\n",
    "    try:\n",
    "        x = next(dl)\n",
    "    except StopIteration:\n",
    "        dl = iter(train_dl)\n",
    "        x = next(dl)\n",
    "    \n",
    "    # update progress string\n",
    "    if gs:\n",
    "        # tn = accuracy anyways\n",
    "        _desc = \" | \".join([\n",
    "            f\"Tr Ac: {np.mean(train_acc[-50:]):.3f}\",\n",
    "            f\"Tr Lo: {np.mean(train_loss[-50:]):.3f}\",\n",
    "            f\"Tr F1: {np.mean(train_f1[-50:]):.3f}\",\n",
    "            f\"Te Ac: {np.mean(test_acc[-50:]):.3f}\",\n",
    "            f\"Te Lo: {np.mean(test_loss[-50:]):.3f}\",\n",
    "            f\"Te F1: {np.mean(test_f1[-50:]):.3f}\",\n",
    "        ])\n",
    "        pbar.set_description(_desc)\n",
    "        \n",
    "    # perform training\n",
    "    logits, loss, acc, f1, cm = model(**x)\n",
    "    \n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    train_acc.append(acc)\n",
    "    train_loss.append(loss.item())\n",
    "    train_f1.append(f1)\n",
    "    train_cm.append(cm)\n",
    "    \n",
    "    # test when the time comes\n",
    "    if gs and gs % n_steps_test == 0:\n",
    "        model.eval()\n",
    "        _test_acc = []\n",
    "        _test_loss = []\n",
    "        _test_cm = []\n",
    "        _test_f1 = []\n",
    "        for x in test_dl:\n",
    "            logits, loss, acc, f1, cm = model(**x)\n",
    "            _test_acc.append(acc)\n",
    "            _test_loss.append(loss.item())\n",
    "            _test_cm.append(cm)\n",
    "            _test_f1.append(f1)\n",
    "        test_cm.append(np.mean(_test_cm, 0))\n",
    "        test_acc.append(np.mean(_test_acc))\n",
    "        test_loss.append(np.mean(_test_loss))\n",
    "        test_f1.append(np.mean(_text_f1))\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b8df1d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 10, 18, 19, 36, 37, 41, 42, 43, 44, 45, 55]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    string = '''and while india struggles to shake off the virus the developed world\n",
    "    is taking off the masks the countries who've been able to vaccinate a sizeable number\n",
    "    of people those living in the united states for instance they can ditch the masks at\n",
    "    most public places now if they have taken both the shots'''\n",
    "    tokenized = tokenizer(string, return_tensors = \"pt\")\n",
    "    out = model(**tokenized)\n",
    "    out[out > 0.5] = 1\n",
    "    out[out <= 0.5] = 0\n",
    "    out = out.view(-1).cpu().tolist()\n",
    "    up_tok = [i for i,x in enumerate(out) if x == 1.]\n",
    "up_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2483eeda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"And While India Struggles to shake off the virus The developed world is taking off the masks The Countries who've been able to vaccinate a sizeable number of people Those Living in the united States For Instance They Can ditch the masks at most public places now if They have taken both the shots [SEP]\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [tokenizer.decode(x) for x in tokenized[\"input_ids\"].view(-1)]\n",
    "def autocase(s, d):\n",
    "    auto_cased = []\n",
    "    s = s.view(-1).tolist()[1:]\n",
    "    s = [tokenizer.decode(x) for x in s]\n",
    "    for i,t in enumerate(s):\n",
    "        if i in d:\n",
    "            auto_cased.append(t.capitalize())\n",
    "        else:\n",
    "            auto_cased.append(t)\n",
    "#     auto_cased = auto_cased[1:]\n",
    "\n",
    "    fseq = \"\"\n",
    "    for t in auto_cased:\n",
    "        fseq += t if t[0] == \"#\" else \" \" + t\n",
    "    fseq = re.sub(r\"#\", \"\", fseq.strip())\n",
    "    fseq = re.sub(r\"\\s'\\s\", \"'\", fseq)\n",
    "    fseq = re.sub(r\"\\[\\s\", \"[\", fseq)\n",
    "    fseq = re.sub(r\"\\s\\]\", \"]\", fseq)\n",
    "    return fseq\n",
    "\n",
    "autocase(tokenized[\"input_ids\"], up_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf117b0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML and DL (PyTorch Only) with CUDA 11.1",
   "language": "python",
   "name": "cuda111"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
